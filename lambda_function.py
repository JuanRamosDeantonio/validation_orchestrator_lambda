import json
import logging
import os
from typing import List, Dict, Any, Optional
from dataclasses import dataclass

from app.markdown_rule_binder import MarkdownRuleBinder
from app.markdown_provider import MarkdownConsumer
from app.models import RuleData
from app.s3_reader import S3JsonReader, create_s3_reader
from app.final_rule_grouping import group_rules
from app.prompt_formatter import format_prompts
from app.bedrock_validator import process_prompts_hybrid_optimized as validate_prompts_lambda, generate_report_sync
from app.bedrock_client import run_bedrock_prompt
from app.lambda_invoker import create_lambda_invoker
from app.report_producer import produce_report, report_to_lambda, gather_prompt_results
from app.config import Config
from app.helper import format_rule_violations_report, join_sections, printer_prompt

# Configurar logging para Lambda (CloudWatch)
logger = logging.getLogger()
logger.setLevel(logging.INFO)


@dataclass
class PipelineConfig:
    """ConfiguraciÃ³n del pipeline de validaciÃ³n"""
    repository_url: str
    branch: str
    report_title: str = "Reporte de ValidaciÃ³n"


class ValidationPipeline:
    """
    Pipeline principal para validaciÃ³n de repositorios usando IA.
    
    Este pipeline procesa un repositorio, aplica reglas de validaciÃ³n,
    genera prompts y obtiene validaciones usando AWS Bedrock.
    """
    
    def __init__(self, config: PipelineConfig):
        self.config = config
        self.s3_reader = S3JsonReader()
        self.markdown_provider = MarkdownConsumer()
        self.rules: List[RuleData] = []
        self.groups = []
        self.prompts = []
        self.lambda_invoker  = create_lambda_invoker()
        self.bedrock_region = os.environ.get('BEDROCK_REGION', '')
    
    def execute(self) -> Dict[str, Any]:
        """
        Ejecuta el pipeline completo de validaciÃ³n.
        
        Returns:
            Dict con el resultado de la validaciÃ³n y reporte generado
        """
        try:
            logger.info("ðŸš€ Iniciando pipeline de validaciÃ³n")
            
            # 1. Cargar configuraciÃ³n y reglas
            self._load_validation_rules()
            
            # 2. Procesar estructura del repositorio
            repository_structure = self._process_repository_structure()
            
            # 3. Vincular reglas con archivos
            self._bind_rules_to_files(repository_structure)

            if not (repository_structure.markdown_content and repository_structure.markdown_content.strip()) \
               and not repository_structure.files:
                raise ValueError("No hay contenido de estructura.")
            
            # 4. Agrupar reglas y generar prompts
            self._generate_validation_prompts(repository_structure)
            
            # 5. Ejecutar validaciÃ³n con IA
            validation_result = self._execute_ai_validation()
  
            # 6. Organizar el resultado de los prompts ejecutados
            prompt_results = join_sections(
                gather_prompt_results(validation_result),
                format_rule_violations_report(self.rules)
            )

            escribir_markdown(prompt_results,"reporte_temporal")            

            #6.5. Normalizacion del reporte con IA
            report = run_bedrock_prompt(prompt_results)
            
            # 7. Generacion del reporte
            report_to_lambda(report, self.config.repository_url)

            # 8. eLIMINACION DE TEMPORALES
            delete_temporal_data = Config.DELETE_TEMPORAL_DATA_FOLDER
            if delete_temporal_data:
                create_s3_reader().delete_temporal_data()

            logger.info("âœ… Pipeline ejecutado exitosamente")
            
            return {
                'validation_result': validation_result,
                'report': report,
                'prompts_count': len(self.prompts),
                'rules_count': len(self.rules)
            }
            
        except Exception as e:
            logger.error(f"âŒ Error en pipeline: {str(e)}")
            raise

  
    def _load_validation_rules(self) -> None:
        """Carga las reglas de validaciÃ³n desde S3"""
        logger.info("ðŸ“‹ Cargando reglas de validaciÃ³n desde S3")
        
        try:
            project_type = self._extract_project_type_from_url(self.config.repository_url)

            if project_type == None:
                raise ValueError('El repositorio no es un proyecto valido a analizar')
            s3_response = self.s3_reader.read_rules()
            self.rules = [RuleData(**item) for item in s3_response.data]

            if project_type:
                self.rules = [rule for rule in self.rules if project_type in rule.projects.split(',')]
            
            logger.info(f"âœ… Cargadas {len(self.rules)} reglas de validaciÃ³n")
            
        except Exception as e:
            logger.error(f"âŒ Error cargando reglas: {str(e)}")
            raise
    
    def _extract_project_type_from_url(self, url: str) -> str:
    # Ejemplo, se asume que el nombre del repo estÃ¡ al final despuÃ©s del Ãºltimo slash
        project_types = ['fcd','orch','srv','adp','trv']
        for project_type in project_types:
            if project_type in url:
                return project_type
    
    def _process_repository_structure(self) -> Any:
        """Procesa la estructura del repositorio objetivo"""
        logger.info(f"ðŸ“ Procesando estructura del repositorio: {self.config.repository_url}")
        
        try:
            structure = self.markdown_provider.get_repository_structure_markdown(
                self.config.repository_url
            )
            
            logger.info(f"âœ… Estructura procesada - {len(structure.files)} archivos encontrados")
            return structure
            
        except Exception as e:
            logger.error(f"âŒ Error procesando estructura: {str(e)}")
            raise
    
    def _bind_rules_to_files(self, repository_structure: Any) -> None:
        """Vincula las reglas de validaciÃ³n con los archivos del repositorio"""
        logger.info("ðŸ”— Vinculando reglas con archivos del repositorio")
        
        try:
            runner = MarkdownRuleBinder(self.markdown_provider)
            runner.run(self.rules, repository_structure.files, self.config.repository_url)
            
            # Contar reglas vinculadas
            bound_rules = sum(1 for rule in self.rules if hasattr(rule, 'references') and rule.references and not rule.has_errors())
            runner.get_unique_markdown_paths(self.rules)
            logger.info(f"âœ… {bound_rules} reglas vinculadas con archivos")
            
        except Exception as e:
            logger.error(f"âŒ Error vinculando reglas: {str(e)}")
            raise
    
    def _generate_validation_prompts(self, repository_structure: Any) -> None:
        """Genera los prompts de validaciÃ³n basados en las reglas agrupadas"""
        logger.info("ðŸ“ Generando prompts de validaciÃ³n")
        
        try:
            # Agrupar reglas sin errores
            rules = [r for r in self.rules if not r.has_errors()]
            self.groups = group_rules(rules)
            logger.info(f"ðŸ“Š Reglas agrupadas en {len(self.groups)} grupos")
            
            # Cargar plantillas
            template = self.s3_reader.read_template().data
            template_structure = self.s3_reader.read_template_structure().data
            
            # Definir reemplazos para las plantillas
            replacements = self._create_template_replacements(repository_structure)
            
            # Generar prompts
            self.prompts = format_prompts(self.groups, template, replacements, template_structure)
            
            printer_prompt(self.prompts, Config.IS_PRINT)

            logger.info(f"âœ… {len(self.prompts)} prompts generados")
            
        except Exception as e:
            logger.error(f"âŒ Error generando prompts: {str(e)}")
            raise
    
    def     _create_template_replacements(self, repository_structure: Any) -> Dict[str, Any]:
        """Crea los reemplazos para las plantillas de prompts"""
        return {
            'ESTRUCTURA': repository_structure.markdown_content,
            'REGLAS_ESTRUCTURA': lambda g: "\n".join([
                f"ðŸ“„ {r.description}" for r in g.rules if not r.references
            ]),
            'REGLAS_CONTENIDO': lambda g: "\n".join([
                f"ðŸ“„ {r.description}" for r in g.rules if r.references
            ]),
            'CONTENIDO_ARCHIVOS': lambda g: "\n".join([
                f"\n\nTITULO: {mf.path}\n\nCONTENIDO: {mf.content}" 
                for mf in g.markdownfile if mf
            ]),
        }
    
    def _execute_ai_validation(self) -> Dict[str, Any]:
        """Ejecuta la validaciÃ³n usando IA (AWS Bedrock)"""
        logger.info("ðŸ¤– Ejecutando validaciÃ³n con IA")
        
        try:
            # Preparar prompts en formato requerido
            formatted_prompts = self._prepare_prompts_for_validation()
            
            # Ejecutar validaciÃ³n
            if Config.TEMPORAL_BEDROCK_CONFIG:
                result_id = validate_prompts_lambda(formatted_prompts, aws_region=self.bedrock_region,
                                                aws_access_key=Config.BEDROCK_ACCESS_KEY_ID,
                                                aws_secret_key=Config.BEDROCK_SECRET_ACCESS_KEY)
            else:
                result_id = validate_prompts_lambda(formatted_prompts, aws_region=Config.AWS_REGION)
            
            logger.info(f"âœ… ValidaciÃ³n ejecutada - ID: {result_id.get('job_id', 'N/A')}")
            return result_id
            
        except Exception as e:
            logger.error(f"âŒ Error en validaciÃ³n IA: {str(e)}")
            raise
    
    def _prepare_prompts_for_validation(self) -> List[Dict[str, str]]:
        """
        Convierte los prompts al formato requerido por validate_prompts_lambda
        
        Returns:
            Lista de prompts en formato: [{"id": "prompt_001", "prompt": "..."}]
        """
        return [
            {"id": f"prompt_{i+1:03d}", "prompt": prompt.strip()} 
            for i, prompt in enumerate(self.prompts) 
            if prompt and prompt.strip()
        ]
    
    def _generate_final_report(self, validation_result: Dict[str, Any]) -> str:
        """Genera el reporte final de validaciÃ³n"""
        logger.info("ðŸ“Š Generando reporte final")
        
        try:
            report = generate_report_sync(validation_result, self.config.report_title)
            logger.info("âœ… Reporte generado exitosamente")
            return report
            
        except Exception as e:
            logger.error(f"âŒ Error generando reporte: {str(e)}")
            raise


class ValidationResultAnalyzer:
    """
    Analizador de resultados de validaciÃ³n.
    
    Proporciona mÃ©todos para inspeccionar y mostrar los resultados
    de manera estructurada y comprensible.
    """
    
    def analyze_results(self, validation_result: Dict[str, Any]) -> Dict[str, Any]:
        """
        Analiza y muestra los resultados de validaciÃ³n de forma estructurada
        
        Args:
            validation_result: Resultado de la validaciÃ³n desde AWS Bedrock
            
        Returns:
            Resumen estructurado de los resultados
        """
        logger.info("ðŸ” Analizando resultados de validaciÃ³n")
        
        analysis = {
            'basic_info': self._extract_basic_info(validation_result),
            'performance_metrics': self._extract_performance_metrics(validation_result),
            'detailed_summary': self._extract_detailed_summary(validation_result),
            'ai_responses': self.get_ai_responses(validation_result),
            'error_info': self._extract_error_info(validation_result)
        }
        
        self._log_analysis_summary(analysis)
        return analysis
    
    def _extract_basic_info(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae informaciÃ³n bÃ¡sica del resultado"""
        basic_info = {
            'job_id': result.get('job_id', 'N/A'),
            'status': result.get('status', 'N/A'),
            'processing_mode': result.get('processing_mode', 'N/A'),
            'processing_strategy': result.get('processing_strategy', 'N/A'),
            'used_s3': 's3_info' in result
        }
        
        if 's3_info' in result:
            s3_info = result['s3_info']
            basic_info['s3_bucket'] = s3_info.get('bucket', 'N/A')
            basic_info['s3_input_key'] = s3_info.get('input_key', 'N/A')
        
        return basic_info
    
    def _extract_performance_metrics(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae mÃ©tricas de rendimiento"""
        metrics = {}
        
        if 'performance_metrics' in result:
            perf = result['performance_metrics']
            metrics.update({
                'prompts_per_second': perf.get('prompts_per_second', 0),
                'total_time_minutes': perf.get('total_time_minutes', 0)
            })
        
        if 'summary' in result:
            metrics['summary'] = result['summary']
        
        return metrics
    
    def _extract_detailed_summary(self, result: Dict[str, Any]) -> Dict[str, Any]:
        """Extrae resumen detallado de resultados"""
        if 'results' not in result:
            return {'total_prompts': 0, 'successful_executions': 0, 'failed_executions': 0}
        
        total_prompts = len(result['results'])
        successful = 0
        failed = 0
        
        for prompt_result in result['results']:
            if 'execution' in prompt_result:
                execution = prompt_result['execution']
                if execution.get('execution_successful', False):
                    successful += 1
                else:
                    failed += 1
        
        return {
            'total_prompts': total_prompts,
            'successful_executions': successful,
            'failed_executions': failed,
            'success_rate': (successful / total_prompts * 100) if total_prompts > 0 else 0
        }
    
    def _extract_error_info(self, result: Dict[str, Any]) -> Optional[str]:
        """Extrae informaciÃ³n de errores si existe"""
        return result.get('error')
    
    def _log_analysis_summary(self, analysis: Dict[str, Any]) -> None:
        """Registra un resumen del anÃ¡lisis en los logs"""
        basic = analysis['basic_info']
        summary = analysis['detailed_summary']
        
        logger.info("=" * 60)
        logger.info(f"Job ID: {basic['job_id']}")
        logger.info(f"Estado: {basic['status']}")
        logger.info(f"ðŸš€ Estrategia: {basic['processing_strategy']}")
        logger.info(f"â˜ï¸ UsÃ³ S3: {'SÃ­' if basic['used_s3'] else 'No'}")
        logger.info(f"ðŸ“Š Prompts procesados: {summary['total_prompts']}")
        logger.info(f"âœ… Ejecuciones exitosas: {summary['successful_executions']}")
        logger.info(f"âŒ Ejecuciones fallidas: {summary['failed_executions']}")
        logger.info(f"ðŸ“ˆ Tasa de Ã©xito: {summary['success_rate']:.1f}%")
        
        if analysis['error_info']:
            logger.error(f"âŒ Error: {analysis['error_info']}")
        
        logger.info("=" * 60)
    
    def get_ai_responses(self, validation_result: Dict[str, Any]) -> List[Dict[str, str]]:
        """
        Extrae todas las respuestas de IA de los resultados
        
        Returns:
            Lista de respuestas con su ID correspondiente
        """
        responses = []
        
        if 'results' not in validation_result:
            return responses
        
        for prompt_result in validation_result['results']:
            prompt_id = prompt_result.get('prompt_id', 'unknown')
            
            if 'execution' in prompt_result:
                execution = prompt_result['execution']
                response = execution.get('response', '')
                
                if response:
                    responses.append({
                        'prompt_id': prompt_id,
                        'response': response,
                        'tokens_used': execution.get('tokens_used', 0),
                        'successful': execution.get('execution_successful', False)
                    })
        
        return responses


def _extract_config_from_event(event: Dict[str, Any]) -> PipelineConfig:
    """
    Extrae la configuraciÃ³n del pipeline desde el evento de Lambda
    
    Args:
        event: Evento de Lambda conteniendo los parÃ¡metros
        
    Returns:
        PipelineConfig configurado
        
    Raises:
        ValueError: Si faltan parÃ¡metros requeridos
    """
    # Intentar obtener configuraciÃ³n del evento
    repository_url = event.get('repository_url')
    branch = event.get('branch', 'main')
    report_title = event.get('report_title', 'Reporte de ValidaciÃ³n')
    
    # Si no estÃ¡ en el evento, intentar variables de entorno
    if not repository_url:
        repository_url = os.environ.get('REPOSITORY_URL')
        branch = os.environ.get('BRANCH', branch)
        report_title = os.environ.get('REPORT_TITLE', report_title)
    
    # Validar parÃ¡metros requeridos
    if not repository_url:
        raise ValueError("repository_url es requerido en el evento o variable de entorno REPOSITORY_URL")
    
    return PipelineConfig(
        repository_url=repository_url,
        branch=branch,
        report_title=report_title
    )


def _create_lambda_response(status_code: int, body: Dict[str, Any], headers: Optional[Dict[str, str]] = None) -> Dict[str, Any]:
    """
    Crea una respuesta HTTP estÃ¡ndar para Lambda
    
    Args:
        status_code: CÃ³digo de estado HTTP
        body: Cuerpo de la respuesta
        headers: Headers opcionales
        
    Returns:
        Respuesta formateada para Lambda
    """
    default_headers = {
        'Content-Type': 'application/json',
        'Access-Control-Allow-Origin': '*',
        'Access-Control-Allow-Headers': 'Content-Type',
        'Access-Control-Allow-Methods': 'POST, GET, OPTIONS'
    }
    
    if headers:
        default_headers.update(headers)
    
    return {
        'statusCode': status_code,
        'headers': default_headers,
        'body': json.dumps(body, ensure_ascii=False, indent=2, default=_json_fallback)

    }


def lambda_handler(event, context):
    """
    Handler principal de la funciÃ³n Lambda
    
    Args:
        event: Evento de Lambda con parÃ¡metros de configuraciÃ³n
        context: Contexto de ejecuciÃ³n de Lambda
        
    Returns:
        Respuesta HTTP con los resultados del pipeline
    """
    # InformaciÃ³n del contexto de Lambda
    request_id = context.aws_request_id
    function_name = context.function_name
    
    logger.info(f"ðŸš€ Iniciando Lambda {function_name} - Request ID: {request_id}")
    logger.info(f"ðŸ“‹ Evento recibido: {json.dumps(event, ensure_ascii=False)}")
    
    try:
        # 1. Extraer configuraciÃ³n del evento
        config = _extract_config_from_event(event)
        logger.info(f"âš™ï¸ ConfiguraciÃ³n: {config}")
        
        # 2. Ejecutar pipeline de validaciÃ³n
        pipeline = ValidationPipeline(config)
        pipeline_result = pipeline.execute()
        
        # 3. Analizar resultados
        analyzer = ValidationResultAnalyzer()
        analysis = analyzer.analyze_results(pipeline_result['validation_result'])
        
        # 4. Preparar respuesta exitosa
        response_body = {
            'success': True,
            'request_id': request_id,
            'message': 'Pipeline de validaciÃ³n ejecutado exitosamente',
            'pipeline_summary': {
                'prompts_count': pipeline_result['prompts_count'],
                'rules_count': pipeline_result['rules_count'],
                'job_id': analysis['basic_info']['job_id'],
                'success_rate': analysis['detailed_summary']['success_rate']
            },
            'validation_result': pipeline_result['validation_result'],
            'report': pipeline_result['report'],
            'analysis': analysis,
            'config_used': {
                'repository_url': config.repository_url,
                'branch': config.branch,
                'report_title': config.report_title
            }
        }
        
        logger.info(f"âœ… Pipeline completado exitosamente:")
        logger.info(f"   - {pipeline_result['prompts_count']} prompts procesados")
        logger.info(f"   - {pipeline_result['rules_count']} reglas aplicadas")
        logger.info(f"   - {len(analysis['ai_responses'])} respuestas de IA generadas")
        logger.info(f"   - Tasa de Ã©xito: {analysis['detailed_summary']['success_rate']:.1f}%")
        
        return _create_lambda_response(200, response_body)
        
    except ValueError as ve:
        # Error de configuraciÃ³n/parÃ¡metros
        error_message = f"Error de configuraciÃ³n: {str(ve)}"
        logger.error(f"âš™ï¸ {error_message}")
        
        return _create_lambda_response(400, {
            'success': False,
            'request_id': request_id,
            'error_type': 'ConfigurationError',
            'error_message': error_message,
            'help': 'Verifica que repository_url estÃ© presente en el evento o variable de entorno'
        })
        
    except Exception as e:
        # Error general del pipeline
        error_message = f"Error en pipeline: {str(e)}"
        logger.error(f"ðŸ’¥ {error_message}")
        logger.exception("Detalles del error:")
        
        return _create_lambda_response(500, {
            'success': False,
            'request_id': request_id,
            'error_type': 'PipelineError',
            'error_message': error_message,
            'function_name': function_name
        })
    
def _json_fallback(obj):
    if hasattr(obj, "model_dump"):  # Si es Pydantic
        return obj.model_dump()
    elif hasattr(obj, "__dict__"):  # Si es clase normal
        return obj.__dict__
    else:
        return str(obj)
    


def format_incumplimiento_de_regla(all_rules: List[RuleData]) -> str:
    """
    Retorna un string con el formato:
    **Reglas incumplidas:** N - [id1, id2]
    
    ## Detalle de Incumplimientos
    
    ### Regla <id>: <description>
    - <error 1>
    - <error 2>
    """
    # 1) Calcula total e IDs de reglas con error
    failed_rules = [r for r in all_rules if r.has_errors()]
    total_failed = len(failed_rules)
    failed_ids = ", ".join(r.id for r in failed_rules)

    # 2) Encabezado
    header = f"**Reglas incumplidas:** {total_failed} - [{failed_ids}]\n\n## Detalle de Incumplimientos\n\n"

    # 3) Bloque de la regla solicitada
    for rule in failed_rules:
        title = f"### Regla {rule.id}: {rule.description or rule.documentation or ''}".rstrip()
        errors = rule.errors

        if not errors:
            body = f"{title}\n- Sin errores registrados."
        else:
            body = title + "\n" + "\n".join(f"- {msg}" for msg in errors)

        return header + body
    
def escribir_markdown(contenido, nombre_archivo):
    with open(nombre_archivo, 'w', encoding='utf-8') as archivo:
        archivo.write(contenido)
